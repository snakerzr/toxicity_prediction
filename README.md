# Классификация токсичных комментариев

Задача: обучить модель, классифицирующую токсичность комментариев с `f1 score` выше `.78`.

**Данные:**  
159292 строк с комменатриями + таргет (токсичный или нет)

Данные находятся в файле `toxic_comments.csv`. Столбец *text* в нём содержит текст комментария, а *toxic* — целевой признак.

Используемые модели:
* Предобученный `toxic-bert` (`f1_score` на валидационных = `0.93`, на тестовых = `0.94`)
* Finetuned distilbert (`f1_score = 0.78`)
* Логистическая регрессия и lgbm обученные на эмбедингах distilbert показали `70` и `66`.